# DeBERTa Models

This module provides an end-to-end pipeline for training and evaluating transformer-based models using the **DeBERTa (Decoding-enhanced BERT with disentangled attention)** architecture.  
DeBERTa processes text using deep contextual embeddings learned from large-scale pretraining, allowing it to capture subtle semantic nuances, long-range dependencies, and complex sentiment patterns that simpler models may miss.
In our project, DeBERTa enables highly accurate tweet sentiment classification by leveraging powerful contextual understanding rather than relying solely on surface-level word representations.


## In local

### Preprocessing

Split the datasets into training and validation sets:
```bash
poetry run python -m tsc.preprocess.split_dataset
```

### Train Model
This module assumes that the training and validation sets (`tweets_train.csv` and `tweets_val.csv`) have already been generated by the project’s splitting pipeline.
For instructions on how to produce these files, please refer to the root [README](../../../../README.md).

Tokenize the datasets and fine-tune the DeBERTa transformer:
```bash
poetry run python -m tsc.models.deberta.train_deberta
```

All trained models, tokenizers, and checkpoints are saved inside the `artifacts/` directory.
This directory is ignored by version control to prevent large files from polluting the repository.

### Generate predictions

Generate predictions for `test_data.txt` with the fine-tuned DeBERTa model:
```bash
poetry run python -m tsc.models.deberta.predict_deberta
```

This produces a `predictions_deberta.csv` file in the `results/` folder, matching the AIcrowd submission format.


### On Google Colab
It is also possible to train the model and predict using Google Colab.  

The file `train_deberta.ipynb` generates an environment on the connected Drive.  
The file `test_data.txt`, along with the training and validation sets must be placed inside the folder `/content/drive/MyDrive/data`.

The submission predicted from the best model is downloaded at the end of the run.

## Best Model Performance

Between the models deberta-v3-base and deberta-v3-large, the best result achieved was observed on the model **microsoft/deberta-v3-large**, as it was offering the best balance between accuracy and robustness.

## Folder Structure

```text
deberta/
├── __init__.py                # Makes this directory a Python package
├── README.md                  # (this file)
├── predict_deberta.py         # Predict sentiment using a trained DeBERTa model
├── train_deberta.py           # Train DeBERTa on the dataset
├── deberta.ipynb              # Train and predict using DeBERTa on Google Colab.
|
└── artifacts/                 # Folder containing trained models and tokenizers (ignored)
```