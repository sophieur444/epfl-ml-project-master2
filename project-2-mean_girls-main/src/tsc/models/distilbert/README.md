# DistilBERT Models

This module provides a lightweight end-to-end pipeline for training and evaluating transformer-based models using the **DistilBERT (Distilled BERT)** architecture.
DistilBERT retains most of BERT’s contextual understanding while being significantly smaller and faster, making it especially suitable for large-scale tweet classification or environments with limited computational resources.
In our project, DistilBERT enables efficient yet reliable sentiment analysis by capturing the essential semantic information of each tweet without the heavy computational cost of full-sized transformer models.


## In local

### Preprocessing

Split the datasets into training and validation sets:
```bash
poetry run python -m tsc.preprocess.split_dataset
```

### Train model
This module assumes that the training and validation sets (`tweets_train.csv` and `tweets_val.csv`) have already been generated by the project’s splitting pipeline.
For instructions on how to produce these files, please refer to the root [README](../../../../README.md).

Tokenize the datasets and fine-tune the DistilBERT transformer:
```bash
poetry run python -m tsc.models.distilbert.train_distilbert
```

All trained models, tokenizers, and checkpoints are saved inside the `artifacts/` directory.
This directory is ignored by version control to prevent large files from polluting the repository.


### Generate predictions

Generate predictions for `test_data.txt` with the fine-tuned DistilBERT model:
```bash
poetry run python -m tsc.models.distilbert.predict_distilbert
```

This produces a `predictions_distilbert.csv` file in the `results/` folder, matching the AIcrowd submission format.


## On Google Colab
It is also possible to train the model using Google Colab.  

The file `distilbert.ipynb` generates an environment on the connected Drive.  
The file `test_data.txt`, along with the training and validation sets must be placed inside the folder `/content/drive/MyDrive/data`.

The submission predicted from the best model is downloaded at the end of the run.


## Best Model Performance

The only model that was trained was the **distilbert-base-uncased**. It was trained with different parameters, and the best result was achieved with a learning rate of 2e-5, a train batch size of 64 and a evaluation batch size of 128.

## Folder Structure

```text
distilbert/
├── __init__.py                # Makes this directory a Python package
├── README.md                  # (this file)
├── predict_distilbert.py      # Predict sentiment using a trained DistilBERT model
├── train_distilbert.py        # Train DistilBERT on the dataset
├── distilbert.ipynb     # Train DistilBERT on the dataset on Google Colab
|
└── artifacts/                 # Folder containing trained models and tokenizers (ignored)
```